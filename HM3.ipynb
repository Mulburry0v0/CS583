{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdJjP1TF7oEf"
   },
   "source": [
    "# Home 3: Build a CNN for image recognition.\n",
    "\n",
    "### Name: Lijin Zhou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ky4NJzl87oEi"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    \n",
    "4. Upload this .HTML file to your Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2019F/blob/master/homework/HM3/HM3.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHYGtYpi7oEj"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vb8ECbje7oEj"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "1w9mIQK97oEk",
    "outputId": "df6a81ca-6515-4588-e6e4-1e014fc25270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n",
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ljeNrPe7oEn"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "bTMkGOhu7oEo",
    "outputId": "554d007d-dd94-4ae7-a42f-c84131e451e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(labels, num_class=10):\n",
    "    results = numpy.zeros((len(labels), num_class))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i,label] = 1.\n",
    "    return results\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2vkm6AI7oEq"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0x8p9H27oEq"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "noR4BeD07oEr",
    "outputId": "a26bec62-99b1-42b7-bc05-9ed6b59a6eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sKKrlVv7oEt"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHeVtpS47oEu"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "69sGKGmz7oEu",
    "outputId": "0bd21140-e4d9-4460-ce9d-23f437d9ab5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 48)        13872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 48)        20784     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 80)        34640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 80)        320       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 80)        57680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 80)        320       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 80)        57680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 80)        320       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 80)        57680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 80)        320       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 80)        57680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 80)        320       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 80)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 80)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         92288     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               64500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,078,470\n",
      "Trainable params: 1,075,006\n",
      "Non-trainable params: 3,464\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout, GlobalMaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(500))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "4Y_tCJij7oEw",
    "outputId": "bf13fea5-eb0f-43de-af54-eb142608a760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-4 \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "W2oRanqu7oEy",
    "outputId": "52cbcbbe-cc78-4b1d-b332-eccdf9df0fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      " - 129s - loss: 1.4386 - acc: 0.4775 - val_loss: 1.4092 - val_acc: 0.5095\n",
      "Epoch 2/100\n",
      " - 124s - loss: 1.2272 - acc: 0.5581 - val_loss: 1.1149 - val_acc: 0.6040\n",
      "Epoch 3/100\n",
      " - 123s - loss: 1.0820 - acc: 0.6139 - val_loss: 0.9553 - val_acc: 0.6627\n",
      "Epoch 4/100\n",
      " - 124s - loss: 0.9734 - acc: 0.6571 - val_loss: 1.1485 - val_acc: 0.6162\n",
      "Epoch 5/100\n",
      " - 122s - loss: 0.8877 - acc: 0.6880 - val_loss: 0.9638 - val_acc: 0.6684\n",
      "Epoch 6/100\n",
      " - 122s - loss: 0.8171 - acc: 0.7143 - val_loss: 0.8501 - val_acc: 0.7103\n",
      "Epoch 7/100\n",
      " - 122s - loss: 0.7495 - acc: 0.7402 - val_loss: 0.7523 - val_acc: 0.7417\n",
      "Epoch 8/100\n",
      " - 123s - loss: 0.6906 - acc: 0.7641 - val_loss: 0.6676 - val_acc: 0.7710\n",
      "Epoch 9/100\n",
      " - 122s - loss: 0.6460 - acc: 0.7794 - val_loss: 0.6562 - val_acc: 0.7755\n",
      "Epoch 10/100\n",
      " - 122s - loss: 0.6033 - acc: 0.7935 - val_loss: 0.7824 - val_acc: 0.7461\n",
      "Epoch 11/100\n",
      " - 122s - loss: 0.5683 - acc: 0.8053 - val_loss: 0.5997 - val_acc: 0.7960\n",
      "Epoch 12/100\n",
      " - 118s - loss: 0.5342 - acc: 0.8175 - val_loss: 0.6975 - val_acc: 0.7710\n",
      "Epoch 13/100\n",
      " - 125s - loss: 0.5068 - acc: 0.8271 - val_loss: 0.5681 - val_acc: 0.8112\n",
      "Epoch 14/100\n",
      " - 123s - loss: 0.4771 - acc: 0.8375 - val_loss: 0.5479 - val_acc: 0.8164\n",
      "Epoch 15/100\n",
      " - 123s - loss: 0.4537 - acc: 0.8461 - val_loss: 0.6554 - val_acc: 0.7942\n",
      "Epoch 16/100\n",
      " - 120s - loss: 0.4335 - acc: 0.8520 - val_loss: 0.5701 - val_acc: 0.8111\n",
      "Epoch 17/100\n",
      " - 125s - loss: 0.4074 - acc: 0.8599 - val_loss: 0.6540 - val_acc: 0.7968\n",
      "Epoch 18/100\n",
      " - 121s - loss: 0.3869 - acc: 0.8680 - val_loss: 0.6417 - val_acc: 0.8029\n",
      "Epoch 19/100\n",
      " - 121s - loss: 0.3720 - acc: 0.8723 - val_loss: 0.6303 - val_acc: 0.8019\n",
      "Epoch 20/100\n",
      " - 123s - loss: 0.3581 - acc: 0.8790 - val_loss: 0.5905 - val_acc: 0.8202\n",
      "Epoch 21/100\n",
      " - 124s - loss: 0.3355 - acc: 0.8854 - val_loss: 0.5974 - val_acc: 0.8175\n",
      "Epoch 22/100\n",
      " - 122s - loss: 0.3247 - acc: 0.8901 - val_loss: 0.5334 - val_acc: 0.8350\n",
      "Epoch 23/100\n",
      " - 122s - loss: 0.3123 - acc: 0.8937 - val_loss: 0.6273 - val_acc: 0.8132\n",
      "Epoch 24/100\n",
      " - 122s - loss: 0.2983 - acc: 0.8967 - val_loss: 0.6089 - val_acc: 0.8156\n",
      "Epoch 25/100\n",
      " - 123s - loss: 0.2952 - acc: 0.8997 - val_loss: 0.5659 - val_acc: 0.8243\n",
      "Epoch 26/100\n",
      " - 124s - loss: 0.2757 - acc: 0.9066 - val_loss: 0.6042 - val_acc: 0.8304\n",
      "Epoch 27/100\n",
      " - 124s - loss: 0.2613 - acc: 0.9097 - val_loss: 0.5932 - val_acc: 0.8339\n",
      "Epoch 28/100\n",
      " - 122s - loss: 0.2527 - acc: 0.9148 - val_loss: 0.5626 - val_acc: 0.8341\n",
      "Epoch 29/100\n",
      " - 123s - loss: 0.2410 - acc: 0.9177 - val_loss: 0.6929 - val_acc: 0.8013\n",
      "Epoch 30/100\n",
      " - 119s - loss: 0.2331 - acc: 0.9206 - val_loss: 0.6938 - val_acc: 0.8127\n",
      "Epoch 31/100\n",
      " - 120s - loss: 0.2254 - acc: 0.9230 - val_loss: 0.6374 - val_acc: 0.8273\n",
      "Epoch 32/100\n",
      " - 122s - loss: 0.2192 - acc: 0.9275 - val_loss: 0.7189 - val_acc: 0.8072\n",
      "Epoch 33/100\n",
      " - 120s - loss: 0.2091 - acc: 0.9283 - val_loss: 0.5438 - val_acc: 0.8432\n",
      "Epoch 34/100\n",
      " - 117s - loss: 0.2033 - acc: 0.9302 - val_loss: 0.5843 - val_acc: 0.8427\n",
      "Epoch 35/100\n",
      " - 122s - loss: 0.1949 - acc: 0.9327 - val_loss: 0.6329 - val_acc: 0.8373\n",
      "Epoch 36/100\n",
      " - 120s - loss: 0.1932 - acc: 0.9348 - val_loss: 0.7159 - val_acc: 0.8266\n",
      "Epoch 37/100\n",
      " - 121s - loss: 0.1812 - acc: 0.9373 - val_loss: 0.6524 - val_acc: 0.8297\n",
      "Epoch 38/100\n",
      " - 121s - loss: 0.1786 - acc: 0.9396 - val_loss: 0.6435 - val_acc: 0.8342\n",
      "Epoch 39/100\n",
      " - 123s - loss: 0.1686 - acc: 0.9437 - val_loss: 0.6480 - val_acc: 0.8369\n",
      "Epoch 40/100\n",
      " - 122s - loss: 0.1706 - acc: 0.9421 - val_loss: 0.6328 - val_acc: 0.8382\n",
      "Epoch 41/100\n",
      " - 124s - loss: 0.1649 - acc: 0.9433 - val_loss: 0.6044 - val_acc: 0.8423\n",
      "Epoch 42/100\n",
      " - 122s - loss: 0.1568 - acc: 0.9478 - val_loss: 0.6786 - val_acc: 0.8345\n",
      "Epoch 43/100\n",
      " - 121s - loss: 0.1505 - acc: 0.9485 - val_loss: 0.6842 - val_acc: 0.8347\n",
      "Epoch 44/100\n",
      " - 121s - loss: 0.1486 - acc: 0.9506 - val_loss: 0.6284 - val_acc: 0.8491\n",
      "Epoch 45/100\n",
      " - 122s - loss: 0.1452 - acc: 0.9517 - val_loss: 0.6339 - val_acc: 0.8496\n",
      "Epoch 46/100\n",
      " - 124s - loss: 0.1416 - acc: 0.9526 - val_loss: 0.6063 - val_acc: 0.8535\n",
      "Epoch 47/100\n",
      " - 121s - loss: 0.1385 - acc: 0.9530 - val_loss: 0.6826 - val_acc: 0.8425\n",
      "Epoch 48/100\n",
      " - 117s - loss: 0.1346 - acc: 0.9548 - val_loss: 0.7852 - val_acc: 0.8317\n",
      "Epoch 49/100\n",
      " - 124s - loss: 0.1278 - acc: 0.9574 - val_loss: 0.6037 - val_acc: 0.8596\n",
      "Epoch 50/100\n",
      " - 124s - loss: 0.1274 - acc: 0.9568 - val_loss: 0.6970 - val_acc: 0.8412\n",
      "Epoch 51/100\n",
      " - 124s - loss: 0.1257 - acc: 0.9583 - val_loss: 0.6719 - val_acc: 0.8471\n",
      "Epoch 52/100\n",
      " - 126s - loss: 0.1220 - acc: 0.9597 - val_loss: 0.7081 - val_acc: 0.8447\n",
      "Epoch 53/100\n",
      " - 126s - loss: 0.1175 - acc: 0.9602 - val_loss: 0.6481 - val_acc: 0.8541\n",
      "Epoch 54/100\n",
      " - 122s - loss: 0.1181 - acc: 0.9605 - val_loss: 0.7607 - val_acc: 0.8324\n",
      "Epoch 55/100\n",
      " - 124s - loss: 0.1169 - acc: 0.9612 - val_loss: 0.7029 - val_acc: 0.8459\n",
      "Epoch 56/100\n",
      " - 124s - loss: 0.1133 - acc: 0.9620 - val_loss: 0.8508 - val_acc: 0.8258\n",
      "Epoch 57/100\n",
      " - 125s - loss: 0.1124 - acc: 0.9633 - val_loss: 0.6613 - val_acc: 0.8512\n",
      "Epoch 58/100\n",
      " - 124s - loss: 0.1047 - acc: 0.9651 - val_loss: 0.7006 - val_acc: 0.8531\n",
      "Epoch 59/100\n",
      " - 122s - loss: 0.1053 - acc: 0.9654 - val_loss: 0.7206 - val_acc: 0.8482\n",
      "Epoch 60/100\n",
      " - 124s - loss: 0.1086 - acc: 0.9646 - val_loss: 0.7101 - val_acc: 0.8521\n",
      "Epoch 61/100\n",
      " - 123s - loss: 0.1042 - acc: 0.9653 - val_loss: 0.7360 - val_acc: 0.8455\n",
      "Epoch 62/100\n",
      " - 125s - loss: 0.1000 - acc: 0.9670 - val_loss: 0.6998 - val_acc: 0.8551\n",
      "Epoch 63/100\n",
      " - 125s - loss: 0.1001 - acc: 0.9670 - val_loss: 0.8487 - val_acc: 0.8280\n",
      "Epoch 64/100\n",
      " - 121s - loss: 0.0987 - acc: 0.9672 - val_loss: 0.7914 - val_acc: 0.8417\n",
      "Epoch 65/100\n",
      " - 124s - loss: 0.0933 - acc: 0.9693 - val_loss: 0.7603 - val_acc: 0.8473\n",
      "Epoch 66/100\n",
      " - 124s - loss: 0.0976 - acc: 0.9691 - val_loss: 0.7605 - val_acc: 0.8536\n",
      "Epoch 67/100\n",
      " - 124s - loss: 0.0943 - acc: 0.9695 - val_loss: 0.7326 - val_acc: 0.8524\n",
      "Epoch 68/100\n",
      " - 123s - loss: 0.0961 - acc: 0.9691 - val_loss: 0.7527 - val_acc: 0.8477\n",
      "Epoch 69/100\n",
      " - 122s - loss: 0.0907 - acc: 0.9710 - val_loss: 0.7935 - val_acc: 0.8429\n",
      "Epoch 70/100\n",
      " - 124s - loss: 0.0883 - acc: 0.9705 - val_loss: 0.7936 - val_acc: 0.8447\n",
      "Epoch 71/100\n",
      " - 122s - loss: 0.0913 - acc: 0.9705 - val_loss: 0.7437 - val_acc: 0.8502\n",
      "Epoch 72/100\n",
      " - 123s - loss: 0.0833 - acc: 0.9732 - val_loss: 0.7656 - val_acc: 0.8501\n",
      "Epoch 73/100\n",
      " - 124s - loss: 0.0865 - acc: 0.9713 - val_loss: 0.7727 - val_acc: 0.8506\n",
      "Epoch 74/100\n",
      " - 124s - loss: 0.0860 - acc: 0.9735 - val_loss: 0.8270 - val_acc: 0.8425\n",
      "Epoch 75/100\n",
      " - 125s - loss: 0.0804 - acc: 0.9738 - val_loss: 0.7401 - val_acc: 0.8542\n",
      "Epoch 76/100\n",
      " - 125s - loss: 0.0863 - acc: 0.9732 - val_loss: 0.7970 - val_acc: 0.8472\n",
      "Epoch 77/100\n",
      " - 124s - loss: 0.0828 - acc: 0.9747 - val_loss: 0.7857 - val_acc: 0.8462\n",
      "Epoch 78/100\n",
      " - 123s - loss: 0.0786 - acc: 0.9753 - val_loss: 0.7603 - val_acc: 0.8554\n",
      "Epoch 79/100\n",
      " - 123s - loss: 0.0806 - acc: 0.9741 - val_loss: 0.7545 - val_acc: 0.8572\n",
      "Epoch 80/100\n",
      " - 121s - loss: 0.0802 - acc: 0.9740 - val_loss: 0.7835 - val_acc: 0.8491\n",
      "Epoch 81/100\n",
      " - 123s - loss: 0.0752 - acc: 0.9761 - val_loss: 0.8321 - val_acc: 0.8463\n",
      "Epoch 82/100\n",
      " - 122s - loss: 0.0809 - acc: 0.9749 - val_loss: 0.8270 - val_acc: 0.8477\n",
      "Epoch 83/100\n",
      " - 125s - loss: 0.0782 - acc: 0.9758 - val_loss: 0.7533 - val_acc: 0.8523\n",
      "Epoch 84/100\n",
      " - 120s - loss: 0.0748 - acc: 0.9771 - val_loss: 0.8226 - val_acc: 0.8555\n",
      "Epoch 85/100\n",
      " - 118s - loss: 0.0759 - acc: 0.9762 - val_loss: 0.7776 - val_acc: 0.8511\n",
      "Epoch 86/100\n",
      " - 121s - loss: 0.0742 - acc: 0.9769 - val_loss: 0.7436 - val_acc: 0.8588\n",
      "Epoch 87/100\n",
      " - 124s - loss: 0.0727 - acc: 0.9763 - val_loss: 0.8248 - val_acc: 0.8514\n",
      "Epoch 88/100\n",
      " - 122s - loss: 0.0758 - acc: 0.9771 - val_loss: 0.7268 - val_acc: 0.8584\n",
      "Epoch 89/100\n",
      " - 122s - loss: 0.0661 - acc: 0.9785 - val_loss: 0.8195 - val_acc: 0.8550\n",
      "Epoch 90/100\n",
      " - 123s - loss: 0.0703 - acc: 0.9790 - val_loss: 0.7796 - val_acc: 0.8552\n",
      "Epoch 91/100\n",
      " - 125s - loss: 0.0684 - acc: 0.9784 - val_loss: 0.9315 - val_acc: 0.8428\n",
      "Epoch 92/100\n",
      " - 123s - loss: 0.0693 - acc: 0.9788 - val_loss: 0.7352 - val_acc: 0.8634\n",
      "Epoch 93/100\n",
      " - 122s - loss: 0.0715 - acc: 0.9780 - val_loss: 0.7590 - val_acc: 0.8566\n",
      "Epoch 94/100\n",
      " - 122s - loss: 0.0702 - acc: 0.9788 - val_loss: 0.8210 - val_acc: 0.8507\n",
      "Epoch 95/100\n",
      " - 124s - loss: 0.0687 - acc: 0.9795 - val_loss: 0.8275 - val_acc: 0.8520\n",
      "Epoch 96/100\n",
      " - 124s - loss: 0.0626 - acc: 0.9808 - val_loss: 0.8091 - val_acc: 0.8553\n",
      "Epoch 97/100\n",
      " - 124s - loss: 0.0634 - acc: 0.9804 - val_loss: 0.7936 - val_acc: 0.8582\n",
      "Epoch 98/100\n",
      " - 125s - loss: 0.0656 - acc: 0.9795 - val_loss: 0.8580 - val_acc: 0.8527\n",
      "Epoch 99/100\n",
      " - 122s - loss: 0.0699 - acc: 0.9792 - val_loss: 0.8489 - val_acc: 0.8556\n",
      "Epoch 100/100\n",
      " - 123s - loss: 0.0648 - acc: 0.9802 - val_loss: 0.8563 - val_acc: 0.8492\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=32, epochs=100, validation_data=(x_val, y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "xJMt_aS97oE0",
    "outputId": "35b7bd0f-6925-4efe-8c33-66dbb7a6d5a7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB7vpcQyk7kdZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PEPLnlSh7oE2"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zg6MWUL47oE3"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JkRC9wK7oE3"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1E-4 \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phLD4MJSEDO2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False,  \n",
    "        rotation_range=10,  \n",
    "        width_shift_range=0.2,  \n",
    "        horizontal_flip=True, \n",
    "        vertical_flip=False)  \n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xsIAh6Zv7oE6",
    "outputId": "b67fe0b1-b7a9-4b41-e895-54678062313d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 159s - loss: 0.6511 - acc: 0.8347\n",
      "Epoch 2/100\n",
      " - 156s - loss: 0.5099 - acc: 0.8482\n",
      "Epoch 3/100\n",
      " - 156s - loss: 0.4777 - acc: 0.8550\n",
      "Epoch 4/100\n",
      " - 156s - loss: 0.4526 - acc: 0.8611\n",
      "Epoch 5/100\n",
      " - 156s - loss: 0.4392 - acc: 0.8628\n",
      "Epoch 6/100\n",
      " - 155s - loss: 0.4246 - acc: 0.8678\n",
      "Epoch 7/100\n",
      " - 156s - loss: 0.4205 - acc: 0.8675\n",
      "Epoch 8/100\n",
      " - 155s - loss: 0.4019 - acc: 0.8735\n",
      "Epoch 9/100\n",
      " - 156s - loss: 0.3970 - acc: 0.8753\n",
      "Epoch 10/100\n",
      " - 156s - loss: 0.3869 - acc: 0.8774\n",
      "Epoch 11/100\n",
      " - 155s - loss: 0.3772 - acc: 0.8820\n",
      "Epoch 12/100\n",
      " - 156s - loss: 0.3691 - acc: 0.8832\n",
      "Epoch 13/100\n",
      " - 156s - loss: 0.3616 - acc: 0.8852\n",
      "Epoch 14/100\n",
      " - 156s - loss: 0.3592 - acc: 0.8885\n",
      "Epoch 15/100\n",
      " - 156s - loss: 0.3526 - acc: 0.8873\n",
      "Epoch 16/100\n",
      " - 156s - loss: 0.3531 - acc: 0.8870\n",
      "Epoch 17/100\n",
      " - 157s - loss: 0.3458 - acc: 0.8901\n",
      "Epoch 18/100\n",
      " - 157s - loss: 0.3417 - acc: 0.8923\n",
      "Epoch 19/100\n",
      " - 157s - loss: 0.3361 - acc: 0.8922\n",
      "Epoch 20/100\n",
      " - 156s - loss: 0.3348 - acc: 0.8935\n",
      "Epoch 21/100\n",
      " - 158s - loss: 0.3227 - acc: 0.8971\n",
      "Epoch 22/100\n",
      " - 157s - loss: 0.3235 - acc: 0.8989\n",
      "Epoch 23/100\n",
      " - 156s - loss: 0.3201 - acc: 0.8960\n",
      "Epoch 24/100\n",
      " - 157s - loss: 0.3141 - acc: 0.9016\n",
      "Epoch 25/100\n",
      " - 157s - loss: 0.3126 - acc: 0.9018\n",
      "Epoch 26/100\n",
      " - 156s - loss: 0.3034 - acc: 0.9039\n",
      "Epoch 27/100\n",
      " - 156s - loss: 0.3051 - acc: 0.9028\n",
      "Epoch 28/100\n",
      " - 156s - loss: 0.3074 - acc: 0.9026\n",
      "Epoch 29/100\n",
      " - 156s - loss: 0.3025 - acc: 0.9039\n",
      "Epoch 30/100\n",
      " - 157s - loss: 0.2938 - acc: 0.9059\n",
      "Epoch 31/100\n",
      " - 157s - loss: 0.2906 - acc: 0.9074\n",
      "Epoch 32/100\n",
      " - 157s - loss: 0.2879 - acc: 0.9077\n",
      "Epoch 33/100\n",
      " - 156s - loss: 0.2842 - acc: 0.9089\n",
      "Epoch 34/100\n",
      " - 156s - loss: 0.2873 - acc: 0.9085\n",
      "Epoch 35/100\n",
      " - 156s - loss: 0.2821 - acc: 0.9110\n",
      "Epoch 36/100\n",
      " - 157s - loss: 0.2773 - acc: 0.9113\n",
      "Epoch 37/100\n",
      " - 156s - loss: 0.2764 - acc: 0.9126\n",
      "Epoch 38/100\n",
      " - 156s - loss: 0.2767 - acc: 0.9127\n",
      "Epoch 39/100\n",
      " - 156s - loss: 0.2707 - acc: 0.9139\n",
      "Epoch 40/100\n",
      " - 157s - loss: 0.2692 - acc: 0.9152\n",
      "Epoch 41/100\n",
      " - 157s - loss: 0.2647 - acc: 0.9151\n",
      "Epoch 42/100\n",
      " - 156s - loss: 0.2678 - acc: 0.9157\n",
      "Epoch 43/100\n",
      " - 156s - loss: 0.2598 - acc: 0.9184\n",
      "Epoch 44/100\n",
      " - 156s - loss: 0.2627 - acc: 0.9174\n",
      "Epoch 45/100\n",
      " - 157s - loss: 0.2589 - acc: 0.9186\n",
      "Epoch 46/100\n",
      " - 156s - loss: 0.2498 - acc: 0.9212\n",
      "Epoch 47/100\n",
      " - 156s - loss: 0.2469 - acc: 0.9214\n",
      "Epoch 48/100\n",
      " - 157s - loss: 0.2492 - acc: 0.9208\n",
      "Epoch 49/100\n",
      " - 156s - loss: 0.2498 - acc: 0.9204\n",
      "Epoch 50/100\n",
      " - 157s - loss: 0.2484 - acc: 0.9212\n",
      "Epoch 51/100\n",
      " - 156s - loss: 0.2433 - acc: 0.9223\n",
      "Epoch 52/100\n",
      " - 156s - loss: 0.2429 - acc: 0.9218\n",
      "Epoch 53/100\n",
      " - 157s - loss: 0.2416 - acc: 0.9239\n",
      "Epoch 54/100\n",
      " - 157s - loss: 0.2428 - acc: 0.9252\n",
      "Epoch 55/100\n",
      " - 156s - loss: 0.2400 - acc: 0.9253\n",
      "Epoch 56/100\n",
      " - 156s - loss: 0.2349 - acc: 0.9264\n",
      "Epoch 57/100\n",
      " - 155s - loss: 0.2351 - acc: 0.9254\n",
      "Epoch 58/100\n",
      " - 157s - loss: 0.2334 - acc: 0.9249\n",
      "Epoch 59/100\n",
      " - 156s - loss: 0.2281 - acc: 0.9282\n",
      "Epoch 60/100\n",
      " - 156s - loss: 0.2319 - acc: 0.9279\n",
      "Epoch 61/100\n",
      " - 155s - loss: 0.2268 - acc: 0.9272\n",
      "Epoch 62/100\n",
      " - 156s - loss: 0.2242 - acc: 0.9284\n",
      "Epoch 63/100\n",
      " - 156s - loss: 0.2243 - acc: 0.9301\n",
      "Epoch 64/100\n",
      " - 155s - loss: 0.2199 - acc: 0.9304\n",
      "Epoch 65/100\n",
      " - 156s - loss: 0.2184 - acc: 0.9304\n",
      "Epoch 66/100\n",
      " - 156s - loss: 0.2199 - acc: 0.9301\n",
      "Epoch 67/100\n",
      " - 156s - loss: 0.2215 - acc: 0.9309\n",
      "Epoch 68/100\n",
      " - 156s - loss: 0.2166 - acc: 0.9320\n",
      "Epoch 69/100\n",
      " - 155s - loss: 0.2192 - acc: 0.9308\n",
      "Epoch 70/100\n",
      " - 155s - loss: 0.2157 - acc: 0.9328\n",
      "Epoch 71/100\n",
      " - 156s - loss: 0.2122 - acc: 0.9325\n",
      "Epoch 72/100\n",
      " - 157s - loss: 0.2125 - acc: 0.9347\n",
      "Epoch 73/100\n",
      " - 157s - loss: 0.2110 - acc: 0.9333\n",
      "Epoch 74/100\n",
      " - 157s - loss: 0.2053 - acc: 0.9360\n",
      "Epoch 75/100\n",
      " - 156s - loss: 0.2095 - acc: 0.9346\n",
      "Epoch 76/100\n",
      " - 155s - loss: 0.2074 - acc: 0.9359\n",
      "Epoch 77/100\n",
      " - 155s - loss: 0.2104 - acc: 0.9357\n",
      "Epoch 78/100\n",
      " - 156s - loss: 0.2033 - acc: 0.9370\n",
      "Epoch 79/100\n",
      " - 157s - loss: 0.2053 - acc: 0.9362\n",
      "Epoch 80/100\n",
      " - 156s - loss: 0.2063 - acc: 0.9366\n",
      "Epoch 81/100\n",
      " - 157s - loss: 0.2015 - acc: 0.9378\n",
      "Epoch 82/100\n",
      " - 157s - loss: 0.1985 - acc: 0.9386\n",
      "Epoch 83/100\n",
      " - 156s - loss: 0.1919 - acc: 0.9411\n",
      "Epoch 84/100\n",
      " - 157s - loss: 0.2000 - acc: 0.9386\n",
      "Epoch 85/100\n",
      " - 157s - loss: 0.2012 - acc: 0.9373\n",
      "Epoch 86/100\n",
      " - 156s - loss: 0.1946 - acc: 0.9393\n",
      "Epoch 87/100\n",
      " - 156s - loss: 0.1942 - acc: 0.9399\n",
      "Epoch 88/100\n",
      " - 156s - loss: 0.1950 - acc: 0.9394\n",
      "Epoch 89/100\n",
      " - 156s - loss: 0.1906 - acc: 0.9400\n",
      "Epoch 90/100\n",
      " - 156s - loss: 0.1911 - acc: 0.9406\n",
      "Epoch 91/100\n",
      " - 155s - loss: 0.1856 - acc: 0.9421\n",
      "Epoch 92/100\n",
      " - 156s - loss: 0.1897 - acc: 0.9421\n",
      "Epoch 93/100\n",
      " - 155s - loss: 0.1934 - acc: 0.9403\n",
      "Epoch 94/100\n",
      " - 155s - loss: 0.1863 - acc: 0.9424\n",
      "Epoch 95/100\n",
      " - 156s - loss: 0.1877 - acc: 0.9417\n",
      "Epoch 96/100\n",
      " - 156s - loss: 0.1854 - acc: 0.9424\n",
      "Epoch 97/100\n",
      " - 156s - loss: 0.1840 - acc: 0.9430\n",
      "Epoch 98/100\n",
      " - 156s - loss: 0.1847 - acc: 0.9426\n",
      "Epoch 99/100\n",
      " - 157s - loss: 0.1841 - acc: 0.9424\n",
      "Epoch 100/100\n",
      " - 156s - loss: 0.1865 - acc: 0.9434\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(x_train, y_train_vec, batch_size = 32), epochs= 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oduQNWCV7oE8"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "z5xEgNH57oE9",
    "outputId": "06297565-0def-4248-d70d-6338282c0720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 572us/step\n",
      "loss = 0.4483616499722004\n",
      "accuracy = 0.8774\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1xsSTK27oE_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HM3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
